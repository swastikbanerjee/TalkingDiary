{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M_ES-bfSPoUo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "raw_txt = []\n",
        "filename = \"user1.txt\"\n",
        "# Opening the file and reading its contents\n",
        "with open(filename, 'r', encoding='utf-8') as file:\n",
        "    # Read the contents of the file into a string\n",
        "    file_contents = file.read()\n",
        "    # Append the string to the list\n",
        "    raw_txt.append(file_contents)\n",
        "# Now, raw_txt contains the contents of \"user1.txt\" as a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o3mOUESuPoUr",
        "outputId": "8bd40877-ddc1-4b17-8141-94210395f59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"This is my first day writing this personal diary of mine. I plan to share all my secrets with you (the diary). I hope to carry out deep conversations with you. I'm rooting for you to give me valuable advices along the way.\"]\n"
          ]
        }
      ],
      "source": [
        "print(raw_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bFZEt0y3PoUt",
        "outputId": "66dda231-d8ec-4015-a7e1-551df19e0eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['first', 'day', 'writing', 'personal', 'diary', 'mine', 'plan', 'share', 'secrets', 'diary', 'hope', 'carry', 'deep', 'conversations', 'rooting', 'give', 'valuable', 'advices', 'along', 'way']]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Downloading NLTK resources\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemming and lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Initialize stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Preprocessing steps\n",
        "clean_txt = []\n",
        "for text in raw_txt:\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Removing Punctuation, Stopwords, and Numbers\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    # Stemming and Lemmatization\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Removing Special Characters\n",
        "    cleaned_tokens = [re.sub(r'[^a-zA-Z0-9\\s]', '', token) for token in tokens]\n",
        "    # Removing Extra Whitespace\n",
        "    cleaned_tokens = [token.strip() for token in cleaned_tokens if token.strip()]\n",
        "    # Append preprocessed text to the result\n",
        "    clean_txt.append(cleaned_tokens)\n",
        "# Now, clean_txt contains the preprocessed text data\n",
        "print(clean_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-oa_R5ZuPoUu",
        "outputId": "c8e1237f-7532-448a-a040-c1d2ba65400b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vector for 'writing': [-0.00714456  0.00124695 -0.00717579 -0.00224254  0.0037249   0.00583441\n",
            "  0.00120493  0.00210188 -0.00411531  0.00722782 -0.00630779  0.00464432\n",
            " -0.00821532  0.00203472 -0.0049721  -0.00425047 -0.00311208  0.00565532\n",
            "  0.00579809 -0.00497918  0.00077567 -0.00849458  0.00781444  0.00925513\n",
            " -0.00274215  0.00080066  0.00074497  0.00547326 -0.00860741  0.00058077\n",
            "  0.00687346  0.00223033  0.0011257  -0.00932056  0.00848539 -0.00626156\n",
            " -0.0029961   0.00349954 -0.00076982  0.00140666  0.00178326 -0.0068309\n",
            " -0.00972542  0.00904331  0.00619928 -0.00691643  0.00340358  0.00020723\n",
            "  0.00475327 -0.00712293  0.00402952  0.00435081  0.00996157 -0.00447783\n",
            " -0.00138544 -0.00732011 -0.00969798 -0.00908144 -0.00102043 -0.00650703\n",
            "  0.00484654 -0.00616875  0.0025223   0.00073932 -0.0033957  -0.00097949\n",
            "  0.00997966  0.00915139 -0.00446533  0.00908463 -0.00564671  0.00593114\n",
            " -0.00309437  0.00343803  0.00301617  0.00690075 -0.00237569  0.00877494\n",
            "  0.00758841 -0.0095488  -0.00801153 -0.00764082  0.00292814 -0.00279533\n",
            " -0.00692653 -0.00812829  0.00831361  0.00198825 -0.00932888 -0.00479158\n",
            "  0.00313722 -0.00471295  0.00528483 -0.00423592  0.00264843 -0.0080421\n",
            "  0.0062119   0.00482278  0.00079085  0.00301001]\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize and train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=clean_txt, vector_size=100, window=5, min_count=1, sg=0)\n",
        "# Get the word vector for a specific word (e.g., 'writing')\n",
        "word_vector = word2vec_model.wv['writing']\n",
        "# Print the word vector\n",
        "print(\"Word vector for 'writing':\", word_vector)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}